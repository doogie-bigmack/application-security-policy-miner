#!/bin/bash

# damonnator_test.sh - Autonomous E2E Testing Loop for macOS
# Usage: ./damonnator_test.sh [iterations]
# Purpose: Validate features marked as complete in prd.json using E2E tests

ITERATIONS=${1:-50}
REPO="https://github.com/doogie-bigmack/application-security-policy-miner"

echo "üß™ Damonnator Test Loop Starting..."
echo "Will validate completed features with E2E tests"
echo "Iterations: $ITERATIONS"
echo ""

for ((i=1; i<=$ITERATIONS; i++)); do
    echo ""
    echo "========================================"
    echo "Test Iteration $i of $ITERATIONS"
    echo "========================================"
    echo ""

    # Create temp output file
    TEMP_OUTPUT=$(mktemp)

    # Run claude with heredoc focused on testing
    claude --dangerously-skip-permissions -p "@prd.json @e2e-tests.json @test-results.json @progress.txt" << 'PROMPT' | tee "$TEMP_OUTPUT"
You are an autonomous E2E testing engineer. You validate features built by the development loop.

## REPO
https://github.com/doogie-bigmack/application-security-policy-miner

## YOUR MISSION
Validate completed features with automated E2E tests using Claude's Chrome browser integration.

## START EVERY SESSION
1. Read prd.json - find stories with passes: true but test_metadata.test_status != "passed"
2. Read e2e-tests.json - this contains test definitions
3. Read test-results.json - see previous test run results
4. Read progress.txt - understand recent changes
5. Ensure services running: docker-compose up -d
6. Wait 10 seconds for services to be ready

## YOUR JOB
1. **Identify Untested Features:**
   - Find stories in prd.json where passes: true but test_metadata.test_status is "not_tested" or missing
   - Prioritize critical stories first

2. **Run E2E Tests:**
   - Execute: python3 e2e/e2e_runner.py --test-suite e2e-tests.json --prd prd.json --output test-results.json
   - Use Claude's Chrome browser integration to:
     * Navigate to http://localhost:3333
     * Execute test steps from e2e-tests.json
     * Verify expected outcomes
     * Take screenshots on failure

3. **Analyze Results:**
   - Read test-results.json
   - If tests PASS:
     * Update prd.json test_metadata for tested stories:
       - Set test_status: "passed"
       - Set last_tested: [current timestamp]
       - Set failure_count: 0
     * Document success in progress.txt

   - If tests FAIL:
     * DO NOT update passes field in prd.json
     * Update test_metadata:
       - Set test_status: "failed"
       - Set last_tested: [current timestamp]
       - Increment failure_count
       - Set last_failure_reason: [error message]
     * Create detailed failure report in progress.txt:
       - What was tested
       - What failed and why
       - Screenshots captured
       - Recommendations for fix
     * Create GitHub issue with failure details and screenshots

4. **Update Tracking:**
   - Commit test results: git add test-results.json prd.json e2e/screenshots/
   - Commit message: "test: E2E validation results for [story-id]"
   - Push: git push origin main

5. **Continuous Improvement:**
   - If tests are missing for a story, ADD them to e2e-tests.json
   - If selectors changed, UPDATE test definitions
   - If new features added, CREATE new test cases

## E2E TEST EXECUTION PATTERN

For each untested story:

1. **Load Test Definition** from e2e-tests.json
2. **Execute Test Steps** using Claude Chrome tools:
   ```
   - mcp__claude-in-chrome__navigate(url)
   - mcp__claude-in-chrome__computer(action="click", selector=...)
   - mcp__claude-in-chrome__form_input(selector, value)
   - mcp__claude-in-chrome__read_page() to verify elements
   - mcp__claude-in-chrome__computer(action="screenshot") on failure
   ```
3. **Verify Outcomes** match expected results
4. **Capture Diagnostics** on failure:
   - Screenshot
   - Console errors
   - Network requests
   - Page HTML snapshot

## BROWSER VALIDATION EXAMPLES

**Example 1: Test Add Repository**
```
1. Navigate to http://localhost:3333/repositories
2. Click "Add Repository" button
3. Verify modal opens
4. Click "GitHub" integration button
5. Verify GitHub auth flow or connection form appears
6. Take screenshot if any step fails
```

**Example 2: Test Risk Dashboard**
```
1. Navigate to http://localhost:3333/risk
2. Verify metrics cards display (Total Policies, Avg Risk, etc.)
3. Verify risk distribution chart renders
4. Check data is not empty or placeholder
5. Screenshot entire dashboard
```

## RULES
- Only test stories where passes: true in prd.json
- Never modify application code - you only validate
- If test fails, do NOT set passes: false (dev loop handles that)
- Take screenshots on EVERY failure for debugging
- Update test_metadata in prd.json after every test run
- Document all results in progress.txt
- If tests are flaky, run them 2-3 times to confirm
- Always clean up test data after tests complete

## ERROR HANDLING
If E2E infrastructure missing (e2e/ directory doesn't exist):
1. You need to implement Phase 1 first (GitHub issue #53)
2. Create the core infrastructure:
   - e2e/e2e_runner.py
   - e2e/test_executor.py
   - e2e-tests.json
3. Then resume testing

## COMPLETION
When ALL stories in prd.json have test_metadata.test_status == "passed", output exactly:
<promise>ALL_TESTS_COMPLETE</promise>

Now start validating.
PROMPT

    # Check if complete
    if grep -q "<promise>ALL_TESTS_COMPLETE</promise>" "$TEMP_OUTPUT"; then
        echo ""
        echo "üéâ All E2E tests PASSED after $i iterations!"
        rm -f "$TEMP_OUTPUT"
        osascript -e 'display notification "All features validated!" with title "Damonnator Test" sound name "Glass"'
        exit 0
    fi

    # Check for critical failures
    if grep -q "CRITICAL_FAILURE" "$TEMP_OUTPUT"; then
        echo ""
        echo "‚ùå Critical test failure detected. Review logs."
        osascript -e 'display notification "Critical test failure!" with title "Damonnator Test" sound name "Basso"'
        # Don't exit - let it retry next iteration
    fi

    # Clean up temp file
    rm -f "$TEMP_OUTPUT"

    # Brief pause between iterations
    sleep 5

done

echo ""
echo "Finished $ITERATIONS test iterations"
osascript -e 'display notification "Test loop completed" with title "Damonnator Test" sound name "Ping"'
